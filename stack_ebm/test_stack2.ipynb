{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from nets import VanillaNet, NonlocalNet\n",
    "import torchvision.transforms as tr\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "import json\n",
    "\n",
    "def requires_grad(parameters, flag=True):\n",
    "    for p in parameters:\n",
    "        p.requires_grad = flag\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "'''\n",
    "load state dict for ebm0 and ebm1\n",
    "'''\n",
    "EBM = VanillaNet(n_c = 3, n_f = 48).to(device)\n",
    "state_EBM = torch.load('./output_dir/ebm_0.pth', map_location=device)\n",
    "EBM.load_state_dict(state_EBM)\n",
    "\n",
    "D_new = VanillaNet(n_c = 3, n_f = 48).to(device)\n",
    "state_EBM2 = torch.load('./output_dir/ebm_1.pth', map_location=device)\n",
    "D_new.load_state_dict(state_EBM2)\n",
    "\n",
    "'''\n",
    "load config file\n",
    "'''\n",
    "CONFIG_FILE = './config_locker/cifar10_nonconvergent.json'\n",
    "\n",
    "with open(CONFIG_FILE) as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "transform = tr.Compose([tr.Resize(config['im_sz']),\n",
    "                        tr.CenterCrop(config['im_sz']),\n",
    "                        tr.ToTensor(),\n",
    "                        tr.Normalize(tuple(0.5*torch.ones(config['im_ch'])), tuple(0.5*torch.ones(config['im_ch'])))])\n",
    "\n",
    "dataset_fmnist_train = datasets.CIFAR10(root='./data/cifar', train=True, download=True, transform=transform)\n",
    "    \n",
    "loader = torch.utils.data.DataLoader(dataset_fmnist_train, batch_size=128, shuffle=True, drop_last = True, num_workers=int(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_single_ebm(bs, nc, model, device, step_size=0.01, sample_step=200):\n",
    "    #inital with uniform(0,1)\n",
    "    noise = torch.randn(bs, nc, 32,32, device=device)\n",
    "    neg_sample = Variable(2*torch.rand(bs, nc, 32,32)-1).to(device)\n",
    "    neg_sample.requires_grad = True\n",
    "    parameters = model.parameters()\n",
    "\n",
    "    requires_grad(parameters, False)\n",
    "    model.eval()\n",
    "\n",
    "    for k in range(sample_step):\n",
    "        noise.normal_(0, 1)\n",
    "        neg_sample.data.add_(step_size, noise.data)\n",
    "        \n",
    "        dvalue = model(neg_sample)\n",
    "        dvalue.sum().backward()\n",
    "\n",
    "        neg_sample.data.add_(-1, neg_sample.grad.data)\n",
    "\n",
    "        neg_sample.grad.detach_()\n",
    "        neg_sample.grad.zero_()\n",
    "        neg_sample.data.clamp_(-1, 1)\n",
    "            \n",
    "    return neg_sample.detach()\n",
    "\n",
    "def sample_from_stack_2_ebm(model,D_new,bs,device,step_size,sample_step):\n",
    "        neg_sample = sample_from_single_ebm(bs, 3, model, device, step_size=0.01, sample_step=80)\n",
    "        neg_sample = Variable(neg_sample).to(device)\n",
    "        neg_sample.requires_grad = True\n",
    "        requires_grad(D_new.parameters(), False)\n",
    "        requires_grad(model.parameters(), False)\n",
    "        model.eval()\n",
    "        D_new.eval()\n",
    "        \n",
    "        for k in range(sample_step):\n",
    "            noise = torch.randn(neg_sample.shape[0], 3, 32, 32, device=device)\n",
    "\n",
    "            noise.normal_(0, 1)\n",
    "            neg_sample.data.add_(step_size, noise.data)\n",
    "\n",
    "            dvalue = model(neg_sample) + D_new(neg_sample)\n",
    "            dvalue.sum().backward()\n",
    "            #neg_sample.grad.data.clamp_(-0.01, 0.01)\n",
    "            \n",
    "            neg_sample.data.add_(-1, neg_sample.grad.data)\n",
    "\n",
    "            neg_sample.grad.detach_()\n",
    "            neg_sample.grad.zero_()\n",
    "            neg_sample.data.clamp_(-1, 1)\n",
    "\n",
    "        neg_sample = neg_sample.detach()\n",
    "        return neg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d570317195e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                torchvision.utils.save_image(rec2.view(bs,nc,h,w)[j, :, :, :],\n\u001b[0;32m----> 9\u001b[0;31m                                             ('./save/sample/stack_ebm_test/{}.png').format(j+i*bs),normalize=True)\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/lib/python3.7/site-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, filename, nrow, padding, normalize, range, scale_each, pad_value)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n\u001b[0;32m--> 101\u001b[0;31m                      normalize=normalize, range=range, scale_each=scale_each)\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/lib/python3.7/site-packages/torchvision/utils.py\u001b[0m in \u001b[0;36mmake_grid\u001b[0;34m(tensor, nrow, padding, normalize, range, scale_each, pad_value)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mnorm_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mnorm_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/lib/python3.7/site-packages/torchvision/utils.py\u001b[0m in \u001b[0;36mnorm_range\u001b[0;34m(t, range)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mnorm_ip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mnorm_ip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale_each\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#neg_img = sample_stack(EBM,D_new,device,step_size = 0.01, sample_step = 100)\n",
    "for i in range(20):        \n",
    "    bs = 500\n",
    "    rec2 = sample_stack(EBM, D_new, bs, device, step_size=0.01, sample_step=70)\n",
    "    with torch.no_grad():\n",
    "        nc,h,w = rec2.shape[1:] \n",
    "        for j in range(rec2.size(0)):\n",
    "               torchvision.utils.save_image(rec2.view(bs,nc,h,w)[j, :, :, :],\n",
    "                                            ('./save/sample/stack_ebm_test/{}.png').format(j+i*bs),normalize=True)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python fid_score.py ../code/samples/cifar/from_dataset ../opt_agg_code/ebm-anatomy-master/save/sample/stack_ebm_test/ --gpu 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tri(model1,model2,D_new,bs,device,step_size,sample_step):\n",
    "    neg_sample = sample_stack(model1,model2,bs, device, step_size=0.01, sample_step=0)\n",
    "\n",
    "    neg_sample = Variable(neg_sample).to(device)\n",
    "    neg_sample.requires_grad = True\n",
    "\n",
    "    requires_grad(D_new.parameters(), False)\n",
    "    requires_grad(model1.parameters(), False)\n",
    "    requires_grad(model2.parameters(), False)\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    D_new.eval()\n",
    "\n",
    "    for k in range(sample_step):\n",
    "        noise = torch.randn(neg_sample.shape[0], 3, 32, 32, device=device)\n",
    "\n",
    "        noise.normal_(0, 1)\n",
    "        neg_sample.data.add_(step_size, noise.data)\n",
    "\n",
    "        dvalue = model1(neg_sample) + model2(neg_sample) + D_new(neg_sample)\n",
    "        dvalue.sum().backward()\n",
    "        #neg_sample.grad.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "        neg_sample.data.add_(-1, neg_sample.grad.data)\n",
    "\n",
    "        neg_sample.grad.detach_()\n",
    "        neg_sample.grad.zero_()\n",
    "        neg_sample.data.clamp_(-1, 1)\n",
    "\n",
    "    neg_sample = neg_sample.detach()\n",
    "    return neg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(loader):\n",
    "    loader_iter = iter(loader)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(loader_iter)\n",
    "\n",
    "        except StopIteration:\n",
    "            loader_iter = iter(loader)\n",
    "\n",
    "            yield next(loader_iter)\n",
    "\n",
    "def clip_grad(parameters, optimizer):\n",
    "    with torch.no_grad():\n",
    "        for group in optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = optimizer.state[p]\n",
    "\n",
    "                if 'step' not in state or state['step'] < 1:\n",
    "                    continue\n",
    "\n",
    "                step = state['step']\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "                _, beta2 = group['betas']\n",
    "\n",
    "                bound = 3 * torch.sqrt(exp_avg_sq / (1 - beta2 ** step)) + 0.1\n",
    "                p.grad.data.copy_(torch.max(torch.min(p.grad.data, bound), -bound))\n",
    "            \n",
    "def train_tripple(model1,model2, loader,config, device, step_size=0.1, sample_step=100):\n",
    "    D_new = VanillaNet(n_c = 3, n_f = 48).to(device)\n",
    "    \n",
    "    loader = tqdm(enumerate(sample_data(loader)))\n",
    "\n",
    "    noise = torch.randn(128, 3, 32, 32, device=device)\n",
    "\n",
    "    parameters = D_new.parameters()\n",
    "    optimizer = optim.Adam(parameters, lr=0.00005,betas = (0.0,0.999))\n",
    "    \n",
    "    for i, (image) in loader:\n",
    "\n",
    "        image = image[0]\n",
    "        image = image.to(device)\n",
    "\n",
    "        neg_sample = sample_from_stack_2_ebm(model1,model2,128, device, step_size=0.01, sample_step=0)\n",
    "       \n",
    "        neg_sample = Variable(neg_sample).to(device)\n",
    "        neg_sample.requires_grad = True\n",
    "        \n",
    "        requires_grad(parameters, False)\n",
    "        requires_grad(model1.parameters(), False)\n",
    "        requires_grad(model2.parameters(), False)\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        D_new.eval()\n",
    "        \n",
    "        for k in range(sample_step):\n",
    "            if noise.shape[0] != neg_sample.shape[0]:\n",
    "                noise = torch.randn(neg_sample.shape[0], 3, 32, 32, device=device)\n",
    "\n",
    "            noise.normal_(0, 1)\n",
    "            neg_sample.data.add_(step_size, noise.data)\n",
    "\n",
    "            dvalue = model1(neg_sample) + model2(neg_sample) + D_new(neg_sample)\n",
    "            dvalue.sum().backward()\n",
    "            #neg_sample.grad.data.clamp_(-0.01, 0.01)\n",
    "            \n",
    "            neg_sample.data.add_(-1, neg_sample.grad.data)\n",
    "\n",
    "            neg_sample.grad.detach_()\n",
    "            neg_sample.grad.zero_()\n",
    "            neg_sample.data.clamp_(-1, 1)\n",
    "\n",
    "        neg_sample = neg_sample.detach()\n",
    "\n",
    "        requires_grad(parameters, True)\n",
    "        D_new.train()\n",
    "\n",
    "        D_new.zero_grad()\n",
    "        pos_out = D_new(image)\n",
    "        neg_out = D_new(neg_sample)\n",
    "\n",
    "        loss = pos_out - neg_out\n",
    "        loss = loss.mean()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad(parameters, optimizer)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        loader.set_description(f'loss: {loss.item():.5f}')\n",
    "                \n",
    "        if i % 2000 == 0:      \n",
    "            neg_img = neg_sample\n",
    "            vutils.save_image(\n",
    "                neg_img.detach().to('cpu'),\n",
    "                './output_dir/stack3/samples/sample_iter_{}.png'.format(i),\n",
    "                nrow=16,\n",
    "                normalize=True\n",
    "            )\n",
    "        if i % 5000 == 0:\n",
    "            torch.save(D_new.state_dict(), './output_dir/stack3/checkpoint/EBM_iter_{}.pth'.format(i))\n",
    "            torch.save(optimizer.state_dict(), './output_dir/stack3/checkpoint/opt.pth')\n",
    "        if i == 60000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.01524: : 23it [01:50,  4.93s/it] "
     ]
    }
   ],
   "source": [
    "train_tripple(EBM,D_new, loader,config, device=device, step_size=0.01, sample_step=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_tri = VanillaNet(n_c = 3).to(device)\n",
    "state_EBM3 = torch.load('./save/model/single_ebm/EBM_15001.pth', map_location=device)\n",
    "D_tri.load_state_dict(state_EBM3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
